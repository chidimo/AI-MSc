{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90dcdc0a-d4df-4c2d-ac74-e41c1da06f09",
   "metadata": {},
   "source": [
    "# Clustering math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f0b17f-5f17-45bb-98d9-46c4f35053d0",
   "metadata": {},
   "source": [
    "# Decision Tree Manual Calculation Guide\n",
    "\n",
    "## **Goal**\n",
    "Find the best attribute to split on at each node by calculating information gain (or gini) for each attribute.\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Formulas**\n",
    "\n",
    "### **1. Entropy**\n",
    "Measures impurity/uncertainty in a dataset.\n",
    "\n",
    "$$Entropy(S) = -\\sum_{i=1}^{c} p_i \\log_2(p_i)$$\n",
    "\n",
    "Where:\n",
    "- $p_i$ = proportion of class $i$ in dataset $S$\n",
    "- $c$ = number of classes\n",
    "\n",
    "**For binary classification (0/1):**\n",
    "$$Entropy(S) = -p_0 \\log_2(p_0) - p_1 \\log_2(p_1)$$\n",
    "\n",
    "### **2. Information Gain**\n",
    "Reduction in entropy after splitting on attribute $A$.\n",
    "\n",
    "$$Gain(S, A) = Entropy(S) - \\sum_{v \\in Values(A)} \\frac{|S_v|}{|S|} \\times Entropy(S_v)$$\n",
    "\n",
    "Where:\n",
    "- $S$ = parent dataset\n",
    "- $A$ = attribute to split on\n",
    "- $S_v$ = subset where attribute $A$ has value $v$\n",
    "- $\\frac{|S_v|}{|S|}$ = proportion of examples with value $v$\n",
    "\n",
    "### **3. Gini Impurity** (Alternative to Entropy)\n",
    "\n",
    "$$Gini(S) = 1 - \\sum_{i=1}^{c} p_i^2$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Step-by-Step Process**\n",
    "\n",
    "### **Step 1: Calculate Initial Entropy**\n",
    "Count the labels in your dataset.\n",
    "\n",
    "**Example:** 5 positive (label=1), 3 negative (label=0) out of 8 total\n",
    "\n",
    "$$Entropy(S) = -\\frac{5}{8}\\log_2(\\frac{5}{8}) - \\frac{3}{8}\\log_2(\\frac{3}{8})$$\n",
    "\n",
    "### **Step 2: For Each Feature (Attribute)**\n",
    "\n",
    "For each of your 5 features, do the following:\n",
    "\n",
    "**a) Split the data by attribute values**\n",
    "\n",
    "Example: For feature \"Age\" with values {Young, Middle, Senior}:\n",
    "- Group all rows where Age=Young\n",
    "- Group all rows where Age=Middle  \n",
    "- Group all rows where Age=Senior\n",
    "\n",
    "**b) Calculate entropy for each subset**\n",
    "\n",
    "For each group, count positives and negatives, then calculate entropy.\n",
    "\n",
    "Example for Age=Young (2 positive, 1 negative):\n",
    "$$Entropy(S_{Young}) = -\\frac{2}{3}\\log_2(\\frac{2}{3}) - \\frac{1}{3}\\log_2(\\frac{1}{3})$$\n",
    "\n",
    "**c) Calculate weighted average entropy**\n",
    "\n",
    "$$Weighted\\ Entropy = \\sum \\frac{|S_v|}{|S|} \\times Entropy(S_v)$$\n",
    "\n",
    "Example:\n",
    "$$= \\frac{3}{8} \\times Entropy(S_{Young}) + \\frac{3}{8} \\times Entropy(S_{Middle}) + \\frac{2}{8} \\times Entropy(S_{Senior})$$\n",
    "\n",
    "**d) Calculate Information Gain**\n",
    "\n",
    "$$Gain(S, Age) = Entropy(S) - Weighted\\ Entropy$$\n",
    "\n",
    "### **Step 3: Choose Root Node**\n",
    "**Select the attribute with the HIGHEST information gain** as your root node.\n",
    "\n",
    "### **Step 4: Repeat for Each Branch**\n",
    "For each branch created by the split:\n",
    "- If all examples have the same label → **Leaf node** (stop)\n",
    "- If no examples → **Leaf node** with majority class\n",
    "- Otherwise → Repeat Steps 1-3 on the subset\n",
    "\n",
    "---\n",
    "\n",
    "## **Quick Calculation Tips**\n",
    "\n",
    "### **Common log₂ values:**\n",
    "- $\\log_2(1) = 0$\n",
    "- $\\log_2(0.5) = -1$\n",
    "- $\\log_2(0.25) = -2$\n",
    "- $\\log_2(0.125) = -3$\n",
    "\n",
    "### **Special cases:**\n",
    "- If $p_i = 0$: skip that term (since $0 \\log_2(0) = 0$)\n",
    "- If all examples same class: $Entropy = 0$\n",
    "- Maximum entropy (50-50 split): $Entropy = 1$\n",
    "\n",
    "### **Change of base formula:**\n",
    "$$\\log_2(x) = \\frac{\\log_{10}(x)}{\\log_{10}(2)} = \\frac{\\ln(x)}{\\ln(2)}$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Python Helper Functions**\n",
    "```python\n",
    "import math\n",
    "\n",
    "def entropy(counts):\n",
    "    \"\"\"Calculate entropy from class counts\"\"\"\n",
    "    total = sum(counts)\n",
    "    if total == 0:\n",
    "        return 0\n",
    "    \n",
    "    ent = 0\n",
    "    for count in counts:\n",
    "        if count > 0:\n",
    "            p = count / total\n",
    "            ent -= p * math.log2(p)\n",
    "    return ent\n",
    "\n",
    "def information_gain(parent_counts, subsets):\n",
    "    \"\"\"\n",
    "    Calculate information gain\n",
    "    parent_counts: [pos, neg] in parent\n",
    "    subsets: list of [pos, neg] for each subset\n",
    "    \"\"\"\n",
    "    parent_entropy = entropy(parent_counts)\n",
    "    parent_total = sum(parent_counts)\n",
    "    \n",
    "    weighted_entropy = 0\n",
    "    for subset in subsets:\n",
    "        subset_total = sum(subset)\n",
    "        weight = subset_total / parent_total\n",
    "        weighted_entropy += weight * entropy(subset)\n",
    "    \n",
    "    return parent_entropy - weighted_entropy\n",
    "\n",
    "# Example usage:\n",
    "# Parent: 5 Yes, 3 No\n",
    "parent = [5, 3]\n",
    "print(f\"Parent Entropy: {entropy(parent):.4f}\")\n",
    "\n",
    "# After split on attribute: [2 Yes, 1 No], [2 Yes, 1 No], [1 Yes, 1 No]\n",
    "subsets = [[2, 1], [2, 1], [1, 1]]\n",
    "gain = information_gain(parent, subsets)\n",
    "print(f\"Information Gain: {gain:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e112a74c-f328-40d7-b7d1-682b771dd241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For total items 8\n",
      "Proportional entropy = 0.0\n",
      "\n",
      "For total items 8\n",
      "When x = 3, y = 2, entropy = 0.9709505944546686\n",
      "Proportional entropy = 0.6068441215341679\n",
      "\n",
      "0.6068441215341679\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\n",
    "def entropy(x, y):\n",
    "    if x == 0: return 0\n",
    "    summation = x + y\n",
    "    ent_x = -((x/summation) * math.log2(x/summation))\n",
    "    ent_y = -((y/summation) * math.log2(y/summation))\n",
    "\n",
    "    ent = ent_x + ent_y\n",
    "\n",
    "    print(f'When x = {x}, y = {y}, entropy = {ent}')\n",
    "    return ent\n",
    "\n",
    "def entropy_with_prob(t, x, y):\n",
    "    \"\"\"Given x, y, and total items, t, calculate entropy\"\"\"\n",
    "    print(f'For total items {t}')\n",
    "    entr = entropy(x, y)\n",
    "    summation = x + y\n",
    "    proportional_entropy = (summation/t) * entr\n",
    "    print(f'Proportional entropy = {proportional_entropy}\\n')\n",
    "    return proportional_entropy\n",
    "\n",
    "a = entropy_with_prob(8, 0, 3)\n",
    "b = entropy_with_prob(8, 3, 2)\n",
    "# c = entropy_with_prob(8, 1, 2)\n",
    "print(sum([a, b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec09d74a-81bb-4c22-b491-5149d86a5494",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_ml_dl",
   "language": "python",
   "name": "env_ml_dl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
